# config.example.yaml
version: '3.0'  # Version of this configuration file
tasks:
  - name: "summarize"
    system_prompt: "You are an expert ai assistant specialized in summarization."  # System prompt for this task
    query_wrapper_prompt: |
      The document information is below.
      ---------------------
      {context_str}
      ---------------------
      Using the document information and mostly relying on it,
      answer the query.
      Query: {query_str}
      Answer:
    enable_cost_calculator: true
    embed_model: "default"  # ["default", "local:intfloat/multilingual-e5-large"]
    llm_params:
      model: "gpt-3.5-turbo"
      max_tokens: 2048
      temperature: 0.1
    vector_store_params:
      vector_store_type: "LanceDBVectorStore"
      uri: "./.lancedb"
      table_name: "vectors"
      enable_metadata_extraction: false
    service_context_params:
      chunk_size: 1024
      context_window: 4096
    query_engine_params:
      similarity_top_k: 5
  - name: "qa"
    system_prompt: "You are a friendly ai assistant specialized in question answering."  # System prompt for this task
    enable_cost_calculator: true
    embed_model: "default"  # ["default", "local:intfloat/multilingual-e5-large"]
    vector_store_params:
      vector_store_type: "SimpleVectorStore"
    llm_params:
      model: "gpt-4"
    service_context_params:
      chunk_size: 1024
    query_engine_params:
      similarity_top_k: 3
