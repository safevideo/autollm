# config.example.yaml
version: '4.1'  # Version of this configuration file
tasks:
  - name: "summarize"
    llm_model: "gpt-3.5-turbo"
    llm_max_tokens: 256
    llm_temperature: 0.1
    system_prompt: "You are an expert ai assistant specialized in summarization."  # System prompt for this task
    query_wrapper_prompt: |
      The document information is below.
      ---------------------
      {context_str}
      ---------------------
      Using the document information and mostly relying on it,
      answer the query.
      Query: {query_str}
      Answer:
    enable_cost_calculator: true
    embed_model: "default"  # ["default", "local:intfloat/multilingual-e5-large"]
    chunk_size: 512
    chunk_overlap: 200
    similarity_top_k: 6
    response_mode: 'tree_summarize'
    vector_store_type: "SimpleVectorStore"
    enable_keyword_extractor: true
  - name: "qa"
    llm_model: "anthropic.claude-v2"
    llm_max_tokens: 256
    llm_temperature: 0.1
    system_prompt: "You are a friendly ai assistant specialized in question answering."  # System prompt for this task
    enable_cost_calculator: true
    embed_model: "default"  # ["default", "local:intfloat/multilingual-e5-large"]
    vector_store_type: "LanceDBVectorStore"
    lancedb_uri: "./.lancedb"
    lancedb_table_name: "vectors"
    chunk_size: 1024
    chunk_overlap: 200
    similarity_top_k: 2
    response_mode: 'compact'
