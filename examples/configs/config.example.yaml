# config.example.yaml
version: '4.0'  # Version of this configuration file
tasks:
  - name: "summarize"
    llm_model: "gpt-3.5-turbo"
    llm_max_tokens: 2048
    llm_temperature: 0.1
    system_prompt: "You are an expert ai assistant specialized in summarization."  # System prompt for this task
    query_wrapper_prompt: |
      The document information is below.
      ---------------------
      {context_str}
      ---------------------
      Using the document information and mostly relying on it,
      answer the query.
      Query: {query_str}
      Answer:
    enable_cost_calculator: true
    embed_model: "default"  # ["default", "local:intfloat/multilingual-e5-large"]
    chunk_size: 512
    context_window: 4096
    similarity_top_k: 6
    vector_store_type: "LanceDBVectorStore"
    lancedb_uri: "./.lancedb"
    lancedb_table_name: "vectors"
    enable_metadata_extraction: false
  - name: "qa"
    system_prompt: "You are a friendly ai assistant specialized in question answering."  # System prompt for this task
    enable_cost_calculator: true
    embed_model: "default"  # ["default", "local:intfloat/multilingual-e5-large"]
    vector_store_type: "SimpleVectorStore"
    llm_model: "gpt-4"
    chunk_size: 1028
    similarity_top_k: 2
