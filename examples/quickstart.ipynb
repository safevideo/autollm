{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/safevideo/autollm/blob/quickstart-collab/examples/quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Install latest version of autollm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install autollm -Uq\n",
    "!pip install nest-asyncio -Uq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import required modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required functions, classes\n",
    "from autollm import AutoQueryEngine, AutoFastAPI\n",
    "from autollm.utils.document_reading import read_files_as_documents\n",
    "import uvicorn\n",
    "import os\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set your OpenAI API key in order to use default gpt-3.5-turbo model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read Files as Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Either set input directory or input files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:autollm.utils.document_reading:Found 1 'documents'.\n"
     ]
    }
   ],
   "source": [
    "# input_dir = 'data/llama_index/documents'\n",
    "input_files = [\"../README.md\"]\n",
    "recursive = True    # Whether to recursively search in subdirectories\n",
    "\n",
    "documents = read_files_as_documents(input_files=input_files, recursive=recursive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration of AutoQueryEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can completely skip configuration if you want to use default settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ðŸŒŸ **pro tip**: autollm defaults to lancedb as the vector store since it is lightweight, scales from development to production and is 100x cheaper than alternatives!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seekndestroy/mambaforge/envs/aidocs/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing documents into nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 82.13it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  6.19it/s]\n"
     ]
    }
   ],
   "source": [
    "query_engine = AutoQueryEngine.from_parameters(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can configure the AutoQueryEngine to your needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an friendly ai assistant that help users find the most relevant and accurate answers to their questions based on the documents you have access to. When answering the questions, mostly rely on the info in documents.\"\n",
    "\n",
    "query_wrapper_prompt = '''\n",
    "The document information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Using the document information and mostly relying on it,\n",
    "answer the query.\n",
    "Query: {query_str}\n",
    "Answer:\n",
    "'''\n",
    "\n",
    "enable_cost_calculator = True\n",
    "\n",
    "# llm params\n",
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "# vector store params\n",
    "vector_store_type = \"LanceDBVectorStore\"\n",
    "# specific params for LanceDBVectorStore\n",
    "uri = \"tmp/lancedb\" \n",
    "table_name = \"vectors\"\n",
    "\n",
    "# service context params\n",
    "chunk_size = 1024\n",
    "\n",
    "# query engine params\n",
    "similarity_top_k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_params = {\"model\": model}\n",
    "vector_store_params = {\"vector_store_type\": vector_store_type, \"uri\": uri, \"table_name\": table_name}\n",
    "service_context_params = {\"chunk_size\": chunk_size}\n",
    "query_engine_params = {\"similarity_top_k\": similarity_top_k}\n",
    "\n",
    "query_engine = AutoQueryEngine.from_parameters(documents=documents, system_prompt=system_prompt, query_wrapper_prompt=query_wrapper_prompt, enable_cost_calculator=enable_cost_calculator, llm_params=llm_params, vector_store_params=vector_store_params, service_context_params=service_context_params, query_engine_params=query_engine_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ask Anything to Your Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Prompt Token Usage: 743\n",
      "LLM Completion Token Usage: 22\n",
      "LLM Total Token Cost: $0.001159\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"who are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am an expert Q&A system designed to help users find relevant and accurate answers to their questions.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aidocs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
